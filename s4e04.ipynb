{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oWKCmd9cUrlz",
        "5_ucE47MUnLf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### First glance"
      ],
      "metadata": {
        "id": "oWKCmd9cUrlz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Krt8Ii-eKyA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "import pickle\n",
        "from IPython.core.display import HTML\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_log_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  original_train = pd.read_csv('/kaggle/input/ps-4-e-2-abalone-dataset-from-uci/abalone.data', header=None)\n",
        "\n",
        "train = pd.read_csv('/kaggle/input/playground-series-s4e4/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/playground-series-s4e4/test.csv')\n",
        "sample_submission = pd.read_csv('/kaggle/input/playground-series-s4e4/sample_submission.csv')"
      ],
      "metadata": {
        "id": "tTRB6EcXeeJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show all properties on display\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "train.head()"
      ],
      "metadata": {
        "id": "JU-crVUUdOto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(original_train.shape)\n",
        "original_train.columns.tolist()"
      ],
      "metadata": {
        "id": "t-ySl9_udQfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save original dataset into .csv\n",
        "original_train.columns = train.columns[1:]\n",
        "original_train.to_csv('orig.csv', index=False)\n",
        "original_train.tail()"
      ],
      "metadata": {
        "id": "8xShFzKtdTZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_id = test.id\n",
        "\n",
        "train.drop(columns='id', axis=1, inplace=True)\n",
        "test.drop(columns='id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "GRiiGoNI9eju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop_duplicates()\n",
        "\n",
        "# Check whether all duplicates were removed\n",
        "train_duplicates = train[train.duplicated()]\n",
        "print(len(train_duplicates))"
      ],
      "metadata": {
        "id": "uXgal44M9gt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train data: {train.shape}')\n",
        "print(f'Test data: {test.shape}\\n')\n",
        "\n",
        "train_data_percentage = np.round(train.shape[0] / (train.shape[0] + test.shape[0]), 4)\n",
        "print(f'Train data consists of {train_data_percentage * 100}% of all observations')\n",
        "print(f'Test data consists of {(1 - train_data_percentage) * 100}% of all observations')"
      ],
      "metadata": {
        "id": "F_MB86z99izm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.describe().T"
      ],
      "metadata": {
        "id": "CYK0-z729l4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('TRAIN data\\n')\n",
        "print(f'{train.isna().sum()}\\n\\n\\n')\n",
        "\n",
        "print('TEST data\\n')\n",
        "print(test.isna().sum())"
      ],
      "metadata": {
        "id": "PW1M7QRnV5M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop_duplicates()\n",
        "\n",
        "# Check whether all duplicates were removed\n",
        "duplicates = train[train.duplicated()]\n",
        "len(duplicates)"
      ],
      "metadata": {
        "id": "H2p25jOaV6-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.get_dummies(train, drop_first=True, dtype=int)\n",
        "test = pd.get_dummies(test, drop_first=True, dtype=int)"
      ],
      "metadata": {
        "id": "WempgSmZV9YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize': (20, 16)})\n",
        "X.hist(color='orange');"
      ],
      "metadata": {
        "id": "fRLpCw6xWCZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{train.Rings.value_counts()}\\n\\n')\n",
        "print(train.Rings.value_counts() / train.shape[0])"
      ],
      "metadata": {
        "id": "Tyx_MZXGWD0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the train data into X and y\n",
        "X = X.drop(['Rings'], axis=1)\n",
        "y = train.Rings\n",
        "\n",
        "# for column in X.columns.tolist():\n",
        "#     X[column] = X[column].apply(lambda x: (x - X[column].min()) / (X[column].max() - X[column].min()))\n",
        "\n",
        "# # Transform test data\n",
        "# for column in test.columns.tolist():\n",
        "#     test[column] = test[column].apply(lambda x: (x - test[column].min()) / (test[column].max() - test[column].min()))\n",
        "\n",
        "# X.hist(color='LightSeaGreen');"
      ],
      "metadata": {
        "id": "F5eHRcP1WGG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# I figured out best hyperparameters previously\n",
        "best_forest = RandomForestRegressor(\n",
        "    random_state=27,\n",
        ")\n",
        "\n",
        "best_forest.fit(X, y)\n",
        "importance = best_forest.feature_importances_\n",
        "\n",
        "feature_importance = pd.DataFrame(data=importance, index=X.columns, columns=['importance']) \\\n",
        "    .sort_values(ascending=True, by='importance')\n",
        "\n",
        "feature_importance.plot(kind='barh', figsize=(12, 8), color='orange');"
      ],
      "metadata": {
        "id": "pzGDtqJdWQGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.columns)"
      ],
      "metadata": {
        "id": "xoR3TIcKhBeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dataset\n",
        "numeric_columns_train = X.select_dtypes(include=np.number)\n",
        "corr_train = numeric_columns_train.corr(method='pearson')\n",
        "mask_train = np.triu(np.ones_like(corr_train))\n",
        "sns.heatmap(corr_train, annot=True, fmt='.2f', mask=mask_train, cmap='Spectral', cbar=None, linewidth=2)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jrQPUVBHhEGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.drop(['Diameter', 'Whole weight.2'], axis=1)\n",
        "test = test.drop(['Diameter', 'Whole weight.2'], axis=1)"
      ],
      "metadata": {
        "id": "QhjbHC2nhF7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dataset\n",
        "numeric_columns_train = X.select_dtypes(include=np.number)\n",
        "corr_train = numeric_columns_train.corr(method='pearson')\n",
        "mask_train = np.triu(np.ones_like(corr_train))\n",
        "sns.heatmap(corr_train, annot=True, fmt='.2f', mask=mask_train, cmap='coolwarm', cbar=None, linewidth=2)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Sq1DLNUGsm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=27)"
      ],
      "metadata": {
        "id": "ssR3-ZanGttP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# def objective(trial):\n",
        "#     model = RandomForestRegressor(\n",
        "#         n_estimators=trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "# #         criterion=trial.suggest_categorical(\"criterion\", ['poisson', 'absolute_error', 'friedman_mse', 'squared_error']),\n",
        "#         min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 100),\n",
        "#         max_depth=trial.suggest_int(\"max_depth\", 1, 100),\n",
        "#         min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 100),\n",
        "#         random_state=27\n",
        "#     )\n",
        "#     model.fit(X_train, y_train)\n",
        "#     y_pred = model.predict(X_test)\n",
        "#     return np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)\n",
        "\n",
        "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# sampler = TPESampler(seed=27)\n",
        "# study = optuna.create_study(study_name=\"random_forest\", direction=\"maximize\", sampler=sampler)\n",
        "# study.optimize(objective, n_trials=10)\n",
        "\n",
        "# print(\"Number of finished trials: \", len(study.trials))\n",
        "# print(\"Best trial:\")\n",
        "# trial = study.best_trial\n",
        "# print(\"  Value: \", trial.value)\n",
        "# print(\"  Params: \")\n",
        "# for key, value in trial.params.items():\n",
        "#     print(f\"    {key}: {value}\")\n",
        "# print()\n",
        "\n",
        "\"\"\"\n",
        "Number of finished trials:  10\n",
        "Best trial:\n",
        "  Value:  0.164014686713176\n",
        "  Params:\n",
        "    n_estimators: 544\n",
        "    min_samples_leaf: 60\n",
        "    max_depth: 8\n",
        "    min_samples_split: 13\n",
        "\n",
        "CPU times: user 6min 35s, sys: 276 ms, total: 6min 35s\n",
        "Wall time: 6min 35s\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8g-ffZSfGx1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# def objective(trial):\n",
        "#     model = XGBRegressor(\n",
        "#         max_depth=trial.suggest_int('max_depth', 1, 100),\n",
        "#         learning_rate=trial.suggest_float('learning_rate', 0.01, 1.0, log=True),\n",
        "#         n_estimators=trial.suggest_int('n_estimators', 50, 1000),\n",
        "#         min_child_weight=trial.suggest_int('min_child_weight', 1, 10),\n",
        "#         gamma=trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
        "#         subsample=trial.suggest_float('subsample', 0.01, 1.0, log=True),\n",
        "#         colsample_bytree=trial.suggest_float('colsample_bytree', 0.01, 1.0, log=True),\n",
        "#         reg_alpha=trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
        "#         reg_lambda=trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
        "#         use_label_encoder=False,\n",
        "#         random_state=27\n",
        "#     )\n",
        "#     model.fit(X_train, y_train)\n",
        "#     y_pred = model.predict(X_test)\n",
        "#     try:\n",
        "#         return np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
        "#     except Exception as e:\n",
        "#         print(e)\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)\n",
        "\n",
        "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# sampler = TPESampler(seed=27)\n",
        "# study = optuna.create_study(study_name=\"xgb\", direction=\"maximize\", sampler=sampler)\n",
        "# study.optimize(objective, n_trials=20)\n",
        "\n",
        "# print(\"Number of finished trials: \", len(study.trials))\n",
        "# print(\"Best trial:\")\n",
        "# trial = study.best_trial\n",
        "# print(\"  Value: \", trial.value)\n",
        "# print(\"  Params: \")\n",
        "# for key, value in trial.params.items():\n",
        "#     print(f\"    {key}: {value}\")\n",
        "# print()\n",
        "\n",
        "\"\"\"\n",
        "Number of finished trials:  1\n",
        "Best trial:\n",
        "  Value:  0.1775845058982026\n",
        "  Params:\n",
        "    max_depth: 43\n",
        "    learning_rate: 0.42576257222865277\n",
        "    n_estimators: 749\n",
        "    min_child_weight: 9\n",
        "    gamma: 1.1669337024772915e-05\n",
        "    subsample: 0.9097315662154742\n",
        "    colsample_bytree: 0.6114890625963008\n",
        "    reg_alpha: 4.761254082318455e-07\n",
        "    reg_lambda: 0.008602430632882225\n",
        "\n",
        "CPU times: user 24.5 s, sys: 667 ms, total: 25.2 s\n",
        "Wall time: 25.2 s\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "69OAKfBMavoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# def objective(trial):\n",
        "#     model = CatBoostRegressor(\n",
        "#         iterations=trial.suggest_int(\"iterations\", 100, 1000),\n",
        "#         learning_rate=trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True),\n",
        "#         colsample_bylevel=trial.suggest_float(\"colsample_bylevel\", 0.05, 1.0),\n",
        "#         min_data_in_leaf=trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
        "#         depth=trial.suggest_int(\"depth\", 4, 16),\n",
        "#         l2_leaf_reg=trial.suggest_float(\"l2_leaf_reg\", 1e-8, 100.0, log=True),\n",
        "#         verbose=False,\n",
        "#         random_state=27\n",
        "#     )\n",
        "#     model.fit(X_train, y_train)\n",
        "#     y_pred = model.predict(X_test)\n",
        "#     return np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)\n",
        "\n",
        "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# sampler = TPESampler(seed=27)\n",
        "# study = optuna.create_study(study_name=\"catboost\", direction=\"maximize\", sampler=sampler)\n",
        "# study.optimize(objective, n_trials=20)\n",
        "\n",
        "# print(\"Number of finished trials: \", len(study.trials))\n",
        "# print(\"Best trial:\")\n",
        "# trial = study.best_trial\n",
        "# print(\"  Value: \", trial.value)\n",
        "# print(\"  Params: \")\n",
        "\n",
        "# for key, value in trial.params.items():\n",
        "#     print(f\"    {key}: {value}\")\n",
        "# print()\n",
        "\n",
        "\"\"\"\n",
        "Number of finished trials:  20\n",
        "Best trial:\n",
        "  Value:  0.27250015755480833\n",
        "  Params:\n",
        "    iterations: 101\n",
        "    learning_rate: 0.0010172906333606835\n",
        "    colsample_bylevel: 0.4796381789116622\n",
        "    min_data_in_leaf: 42\n",
        "    depth: 13\n",
        "    l2_leaf_reg: 2.895211427077531e-08\n",
        "\n",
        "CPU times: user 18min 10s, sys: 9min 21s, total: 27min 31s\n",
        "Wall time: 13min 5s\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "B-L2DJuJa0KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# def objective(trial):\n",
        "#     model = LGBMRegressor(\n",
        "#         n_estimators=trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "#         max_depth=trial.suggest_int(\"max_depth\", 1, 100),\n",
        "#         learning_rate=trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True),\n",
        "#         verbosity=-1,\n",
        "#         boosting_type=trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
        "#         num_leaves=trial.suggest_int('num_leaves', 2, 256),\n",
        "#         min_child_samples=trial.suggest_int('min_child_samples', 5, 100),\n",
        "#         random_state=27\n",
        "#     )\n",
        "#     model.fit(X_train, y_train)\n",
        "#     y_pred = model.predict(X_test)\n",
        "#     return np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)\n",
        "\n",
        "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# sampler = TPESampler(seed=27)\n",
        "# study = optuna.create_study(study_name=\"lgbm\", direction=\"maximize\", sampler=sampler)\n",
        "# study.optimize(objective, n_trials=20)\n",
        "\n",
        "# print(\"Number of finished trials: \", len(study.trials))\n",
        "# print(\"Best trial:\")\n",
        "# trial = study.best_trial\n",
        "# print(\"  Value: \", trial.value)\n",
        "# print(\"  Params: \")\n",
        "# for key, value in trial.params.items():\n",
        "#     print(f\"    {key}: {value}\")\n",
        "# print()\n",
        "\n",
        "\"\"\"\n",
        "Number of finished trials:  20\n",
        "Best trial:\n",
        "  Value:  0.9971664373669932\n",
        "  Params:\n",
        "    n_estimators: 676\n",
        "    max_depth: 100\n",
        "    learning_rate: 0.0010257989336468524\n",
        "    boosting_type: dart\n",
        "    num_leaves: 37\n",
        "    min_child_samples: 22\n",
        "\n",
        "CPU times: user 38min 41s, sys: 5.25 s, total: 38min 47s\n",
        "Wall time: 38min 50s\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KBdA8mobLyWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_models = [\n",
        "    ('XGBoost', XGBRegressor(\n",
        "        n_estimators=395,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.01,\n",
        "        random_state=27\n",
        "    )),\n",
        "    ('LightGBM', LGBMRegressor(\n",
        "        n_estimators=676,\n",
        "        max_depth=100,\n",
        "        learning_rate=0.0010257989336468524,\n",
        "        boosting_type='dart',\n",
        "        num_leaves=37,\n",
        "        min_child_samples=22,\n",
        "        random_state=27\n",
        "    )),\n",
        "    ('Catboost', CatBoostRegressor(\n",
        "        iterations=101,\n",
        "        learning_rate=0.0010172906333606835,\n",
        "        colsample_bylevel=0.4796381789116622,\n",
        "        min_data_in_leaf=42,\n",
        "        depth=13,\n",
        "        l2_leaf_reg=2.895211427077531e-08,\n",
        "        random_state=27\n",
        "    )),\n",
        "    ('Random_forest', RandomForestRegressor(\n",
        "        n_estimators=544,\n",
        "        min_samples_leaf=60,\n",
        "        max_depth=8,\n",
        "        min_samples_split=13,\n",
        "        random_state=27\n",
        "    ))\n",
        "]"
      ],
      "metadata": {
        "id": "M-Eex9mdL3ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_model = XGBRegressor(\n",
        "    n_estimators=395,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.01,\n",
        "    random_state=27\n",
        ")"
      ],
      "metadata": {
        "id": "cCYkr8Gk4LHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "RandomForestRegressor(\n",
        "    n_estimators=544,\n",
        "    min_samples_leaf=60,\n",
        "    max_depth=8,\n",
        "    min_samples_split=13,\n",
        "    random_state=27\n",
        ")\n",
        "stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "stacking_model.fit(X, y)"
      ],
      "metadata": {
        "id": "_WAO-6A14NCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_val = stacking_model.predict(X_val)\n",
        "\n",
        "rmsle_val = np.sqrt(mean_squared_log_error(y_val, y_pred_val))\n",
        "print(f\"Validation Root mean squared logarithmic error regression loss: {rmsle_val:.8f}\")"
      ],
      "metadata": {
        "id": "0a4iG6YCvBk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = stacking_model.predict(test)\n",
        "y_pred_test[:10]"
      ],
      "metadata": {
        "id": "B5ZQCGIrvE2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'id': sample_submission.id,\n",
        "    'Rings': y_pred_test\n",
        "})\n",
        "\n",
        "submission.to_csv('Kapturov_S4E4_submission.csv', index=False)\n",
        "submission.head(10)"
      ],
      "metadata": {
        "id": "DnTZWDbLUTn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(stacking_model, open(\"Kapturov_stacking_model.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "fe1UTfLZUWHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second approach"
      ],
      "metadata": {
        "id": "5_ucE47MUnLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn import preprocessing\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, Lambda, Concatenate, Add, BatchNormalization, LeakyReLU,ELU\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "EU4hAPIsUqSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/kaggle/input/playground-series-s4e4/train.csv')\n",
        "df_test  = pd.read_csv('/kaggle/input/playground-series-s4e4/test.csv')\n",
        "df_sub = pd.read_csv('/kaggle/input/playground-series-s4e4/sample_submission.csv')"
      ],
      "metadata": {
        "id": "KsCjBfpSUz2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_train['Sex_encoded'] = label_encoder.fit_transform(df_train['Sex'])\n",
        "df_test['Sex_encoded']  = label_encoder.fit_transform(df_test['Sex'])\n",
        "df_train.drop(columns=['Sex'], inplace=True)\n",
        "df_test.drop(columns=['Sex'], inplace=True)"
      ],
      "metadata": {
        "id": "w-ARaLUHZedV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.drop(columns=['id'], inplace=True)\n",
        "df_test.drop(columns=['id'], inplace=True)"
      ],
      "metadata": {
        "id": "AfG8S4uDZg0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.scatterplot(data=df_train, x='Shell weight', y='Rings', hue='Sex', palette='Set1')\n",
        "plt.title('Rings vs. Shell Weight by Sex')\n",
        "plt.xlabel('Shell Weight')\n",
        "plt.ylabel('Rings')\n",
        "plt.legend(title='Sex')\n",
        "plt.gcf().set_facecolor('#DFFF00')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ojsmkyd9Zh0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "sns.boxplot(data=df_train, x='Sex', y='Rings', palette='Set1')\n",
        "plt.title('Age Distribution (Rings) by Sex')\n",
        "plt.xlabel('Sex')\n",
        "plt.ylabel('Rings')\n",
        "plt.gcf().set_facecolor('#FF00FF')\n",
        "plt.xticks(ticks=[0, 1, 2], labels=['Male', 'Female', 'Infant'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8JT0_JKifW8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df_train.corr()\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='viridis', fmt=\".2f\")\n",
        "plt.gcf().set_facecolor('#00FFFF')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vGwCoE4kfZLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = ['Length', 'Diameter', 'Height', 'Whole weight', 'Whole weight.1', 'Whole weight.2', 'Shell weight']\n",
        "num_plots = len(numerical_features)\n",
        "rows = 3\n",
        "cols = math.ceil(num_plots / rows)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
        "\n",
        "for i, feature in enumerate(numerical_features):\n",
        "    row = i // cols\n",
        "    col = i % cols\n",
        "    ax = axes[row, col]\n",
        "    sns.histplot(df_train[feature], kde=True, ax=ax)\n",
        "    ax.set_title(f'Distribution of {feature}')\n",
        "    ax.set_xlabel(feature)\n",
        "    ax.set_ylabel('Frequency')\n",
        "\n",
        "for i in range(num_plots, rows * cols):\n",
        "    row = i // cols\n",
        "    col = i % cols\n",
        "    fig.delaxes(axes[row, col])\n",
        "\n",
        "plt.gcf().set_facecolor('#FFF8DC')  # Set background color of the entire figure\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3UlLrYZrM7vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "sns.boxplot(data=df_train[['Length', 'Diameter', 'Height', 'Whole weight', 'Shell weight']], orient='h', palette='Set3')\n",
        "plt.title('Boxplot of Numerical Features')\n",
        "plt.gcf().set_facecolor('#008080')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LbpSlStnM-VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_counts = df_train['Sex'].value_counts()\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.axis('equal')\n",
        "plt.title('Distribution of Gender')\n",
        "plt.gcf().set_facecolor('#00FF00')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QAN7bUkUNKF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_val = df_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "num_cols = 2\n",
        "num_rows = 8\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(numerical_val):\n",
        "    sns.histplot(df_train[feature], kde=True, ax=axes[i*num_cols])\n",
        "    axes[i*num_cols].set_title(f'{feature} Distribution')\n",
        "    axes[i*num_cols].set_xlabel('')\n",
        "    axes[i*num_cols].set_ylabel('')\n",
        "\n",
        "    if i < len(numerical_val) - 1:\n",
        "        sns.boxplot(x=df_train[feature], ax=axes[i*num_cols+1])\n",
        "        axes[i*num_cols+1].set_title(f'{feature} Boxplot')\n",
        "        axes[i*num_cols+1].set_xlabel('')\n",
        "        axes[i*num_cols+1].set_ylabel('')\n",
        "\n",
        "fig.suptitle('Distribution of Features for Outliers', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.gcf().set_facecolor('lightblue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-XP5ODKrNNxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_train['Sex_encoded'] = label_encoder.fit_transform(df_train['Sex'])\n",
        "df_test['Sex_encodd']  = label_encoder.fit_transform(df_test['Sex'])\n",
        "df_train.drop(columns=['Sex'], inplace=True)\n",
        "df_test.drop(columns=['Sex'], inplace=True)"
      ],
      "metadata": {
        "id": "FAfyGfzIV1vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df_train['Rings']\n",
        "df_train = df_train.drop(['Rings'],axis=1)\n"
      ],
      "metadata": {
        "id": "sg4Hd3GJV6ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_train, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "32ofDeY2V6VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(optimizer='adam', dropout_rate=0.0, learning_rate=0.001, activation='relu', hidden_layers=1):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation=activation, input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    for _ in range(hidden_layers):\n",
        "        model.add(Dense(64, activation=activation))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        optimizer = RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    return model\n",
        "\n",
        "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
        "\n",
        "param_dist = {\n",
        "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
        "    'dropout_rate': np.linspace(0.0, 0.5, 6),\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'hidden_layers': [1, 2, 3]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=20, cv=3, scoring='neg_mean_squared_error', verbose=2)\n",
        "random_search_result = random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best: %f using %s\" % (random_search_result.best_score_, random_search_result.best_params_))\n",
        "means = random_search_result.cv_results_['mean_test_score']\n",
        "stds = random_search_result.cv_results_['std_test_score']\n",
        "params = random_search_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "best_model = random_search_result.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Test MSE:\", mse)\n"
      ],
      "metadata": {
        "id": "sK_7RJyPV7-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Final_Neur = best_model.predict(df_test)"
      ],
      "metadata": {
        "id": "vtKkr1QhWBbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_public1 = pd.read_csv('/kaggle/input/ps4e4-prediction-generalization-regression/submission.csv')\n",
        "df_public2 = pd.read_csv('/kaggle/input/k/jainsanyam10/ps4e4-prediction-generalization-regression/submission.csv')\n",
        "df_public3 = pd.read_csv('/kaggle/input/galaxybillion/bestcnn.csv')\n",
        "df_public4 = pd.read_csv('/kaggle/input/galaxybillion/bestnn.csv')\n",
        "df_public5 = pd.read_csv('/kaggle/input/interspace/voyagerone2.csv')\n",
        "df_public6 = pd.read_csv('/kaggle/input/neuralnetwo/voyageronefront.csv')"
      ],
      "metadata": {
        "id": "zMgnkSlaWEAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub['Rings'] =  Final_Neur*0.001+df_public6['Rings']*0.5 +df_public2['Rings']*0.001+df_public5['Rings']*0.5"
      ],
      "metadata": {
        "id": "C9ebJHB4WFWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub.to_csv('submission.csv', index=False)\n",
        "df_sub['Rings'].hist()"
      ],
      "metadata": {
        "id": "YrmCIM1RWGn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third approach"
      ],
      "metadata": {
        "id": "P8SeTS9pIzsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "tqdm_notebook.get_lock().locks = []\n",
        "from prettytable import PrettyTable\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid', font_scale=1.4)\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from itertools import combinations\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "# !pip install yellowbrick\n",
        "# from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "import random\n",
        "from random import uniform\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error,mean_squared_log_error, mean_absolute_error\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "import math\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "!pip install optuna\n",
        "!pip install cmaes\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "# !pip install catboost\n",
        "!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\n",
        "import lightgbm as lgb\n",
        "!pip install category_encoders\n",
        "from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.linear_model import PassiveAggressiveRegressor, ARDRegression, RidgeCV, ElasticNetCV\n",
        "from sklearn.linear_model import TheilSenRegressor, RANSACRegressor, HuberRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor,ExtraTreesRegressor,GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from catboost import CatBoost, CatBoostRegressor,CatBoostClassifier\n",
        "from catboost import Pool\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.pandas.set_option('display.max_columns',None)"
      ],
      "metadata": {
        "id": "vZhA7JLyWiEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('/kaggle/input/playground-series-s4e4/train.csv')\n",
        "test=pd.read_csv('/kaggle/input/playground-series-s4e4/test.csv')\n",
        "submission=pd.read_csv(\"/kaggle/input/playground-series-s4e4/sample_submission.csv\")\n",
        "\n",
        "original=pd.read_csv(\"/kaggle/input/playgrounds4e04originaldata/Original.csv\")\n",
        "\n",
        "train_copy=train.copy()\n",
        "test_copy=test.copy()\n",
        "\n",
        "# Tag Orignal\n",
        "original[\"original\"]=1\n",
        "train[\"original\"]=0\n",
        "test[\"original\"]=0\n",
        "\n",
        "train.drop(columns=[\"id\"],inplace=True)\n",
        "test.drop(columns=[\"id\"],inplace=True)\n",
        "original.drop(columns=[\"id\"],inplace=True)\n",
        "\n",
        "train=train.rename(columns={'Whole weight':'Whole_weight','Whole weight.1':'Shucked_weight', 'Whole weight.2':'Viscera_weight', 'Shell weight':'Shell_weight'})\n",
        "test=test.rename(columns={'Whole weight':'Whole_weight','Whole weight.1':'Shucked_weight', 'Whole weight.2':'Viscera_weight', 'Shell weight':'Shell_weight'})\n",
        "\n",
        "train=pd.concat([train,original],axis='rows')\n",
        "train.reset_index(inplace=True,drop=True)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "target='Rings'\n",
        "\n",
        "train.head()"
      ],
      "metadata": {
        "id": "cVwK646cI8L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = PrettyTable()\n",
        "\n",
        "table.field_names = ['Column Name', 'Data Type', 'Train Missing %', 'Test Missing %']\n",
        "for column in train.columns:\n",
        "    data_type = str(train[column].dtype)\n",
        "    non_null_count_train= 100-train[column].count()/train.shape[0]*100\n",
        "    if column!=target:\n",
        "        non_null_count_test = 100-test[column].count()/test.shape[0]*100\n",
        "    else:\n",
        "        non_null_count_test=\"NA\"\n",
        "    table.add_row([column, data_type, non_null_count_train,non_null_count_test])\n",
        "print(table)"
      ],
      "metadata": {
        "id": "SqSJZQo9sYB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_0 = train[train['original'] == 0][target]\n",
        "class_1 = train[train['original'] == 1][target]\n",
        "\n",
        "mean_0 = np.mean(class_0)\n",
        "median_0 = np.median(class_0)\n",
        "mean_1 = np.mean(class_1)\n",
        "median_1 = np.median(class_1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "ax.hist(class_0, bins=20, density=True, alpha=0.5, label='Original=0 Histogram')\n",
        "ax.hist(class_1, bins=20, density=True, alpha=0.5, label='Original=1 Histogram')\n",
        "\n",
        "x_values_0 = np.linspace(class_0.min(), class_0.max(), len(class_0))\n",
        "density_values_0 = (1 / (np.sqrt(2 * np.pi) * np.std(class_0))) * np.exp(-0.5 * ((x_values_0 - mean_0) / np.std(class_0))**2)\n",
        "ax.plot(x_values_0, density_values_0, color='red', label='Original=0 Density')\n",
        "\n",
        "x_values_1 = np.linspace(class_1.min(), class_1.max(), len(class_1))\n",
        "density_values_1 = (1 / (np.sqrt(2 * np.pi) * np.std(class_1))) * np.exp(-0.5 * ((x_values_1 - mean_1) / np.std(class_1))**2)\n",
        "ax.plot(x_values_1, density_values_1, color='green', label='Original=1 Density')\n",
        "\n",
        "ax.axvline(mean_0, color='blue', linestyle='dashed', linewidth=2, label='Mean (Original=0)')\n",
        "ax.axvline(median_0, color='green', linestyle='dashed', linewidth=2, label='Median (Original=0)')\n",
        "ax.axvline(mean_1, color='blue', linestyle='dashed', linewidth=2, label='Mean (Original=1)')\n",
        "ax.axvline(median_1, color='red', linestyle='dashed', linewidth=2, label='Median (Original=1)')\n",
        "\n",
        "ax.set_xlabel(target)\n",
        "ax.set_ylabel('Frequency / Density')\n",
        "ax.set_title('Histograms and Density Plots')\n",
        "\n",
        "x_min = min(min(class_0), min(class_1))\n",
        "x_max = max(max(class_0), max(class_1))\n",
        "ax.set_xlim([x_min, x_max])\n",
        "\n",
        "ax.legend(bbox_to_anchor=(1,1),fancybox=False,shadow=False, loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T-e1RML1qMNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_cols=[f for f in train.columns if train[f].dtype in [float,int] and train[f].nunique()>2 and f not in [target]]\n",
        "\n",
        "# Calculate the number of rows needed for the subplots\n",
        "num_rows = (len(cont_cols) + 2) // 3\n",
        "\n",
        "# Create subplots for each continuous column\n",
        "fig, axs = plt.subplots(num_rows, 3, figsize=(15, num_rows*5))\n",
        "\n",
        "# Loop through each continuous column and plot the histograms\n",
        "for i, col in enumerate(cont_cols):\n",
        "    # Determine the range of values to plot\n",
        "    max_val = max(train[col].max(), test[col].max(), original[col].max())\n",
        "    min_val = min(train[col].min(), test[col].min(), original[col].min())\n",
        "    range_val = max_val - min_val\n",
        "\n",
        "    # Determine the bin size and number of bins\n",
        "    bin_size = range_val / 20\n",
        "    num_bins_train = round(range_val / bin_size)\n",
        "    num_bins_test = round(range_val / bin_size)\n",
        "    num_bins_original = round(range_val / bin_size)\n",
        "\n",
        "    # Calculate the subplot position\n",
        "    row = i // 3\n",
        "    col_pos = i % 3\n",
        "\n",
        "    # Plot the histograms\n",
        "    sns.histplot(train[col], ax=axs[row][col_pos], color='orange', kde=True, label='Train', bins=num_bins_train)\n",
        "    sns.histplot(test[col], ax=axs[row][col_pos], color='green', kde=True, label='Test', bins=num_bins_test)\n",
        "    sns.histplot(original[col], ax=axs[row][col_pos], color='blue', kde=True, label='Original', bins=num_bins_original)\n",
        "    axs[row][col_pos].set_title(col)\n",
        "    axs[row][col_pos].set_xlabel('Value')\n",
        "    axs[row][col_pos].set_ylabel('Frequency')\n",
        "    axs[row][col_pos].legend()\n",
        "\n",
        "# Remove any empty subplots\n",
        "if len(cont_cols) % 3 != 0:\n",
        "    for col_pos in range(len(cont_cols) % 3, 3):\n",
        "        axs[-1][col_pos].remove()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MULqtMr3qVDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data=original, vars=cont_cols+[target], hue='Sex')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oODhIVlyK6Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(figsize=(16, 5))\n",
        "sns.violinplot(x='Sex', y=col, data=train)\n",
        "plt.title('Rings Distribution by Sex', fontsize=14)\n",
        "plt.xlabel('Sex', fontsize=12)\n",
        "plt.ylabel('Rings', fontsize=12)\n",
        "sns.despine()\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fdpLFUO9K-Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features=[f for f in test.columns if f!='Sex']\n",
        "corr = train[features].corr()\n",
        "plt.figure(figsize = (8, 6), dpi = 300)\n",
        "mask = np.zeros_like(corr)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "sns.heatmap(corr, mask = mask, cmap = 'magma', annot = True, annot_kws = {'size' : 6})\n",
        "plt.title('Features Correlation Matrix\\n', fontsize = 15, weight = 'bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P7z5kM7Y9Pwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_scaler(train, test, column):\n",
        "    '''\n",
        "    Min Max just based on train might have an issue if test has extreme values, hence changing the denominator uding overall min and max\n",
        "    '''\n",
        "    sc=MinMaxScaler()\n",
        "\n",
        "    max_val=max(train[column].max(),test[column].max())\n",
        "    min_val=min(train[column].min(),test[column].min())\n",
        "\n",
        "    train[column]=(train[column]-min_val)/(max_val-min_val)\n",
        "    test[column]=(test[column]-min_val)/(max_val-min_val)\n",
        "\n",
        "    return train,test\n",
        "\n",
        "def rmse(y1,y2):\n",
        "    ''' RMSE Evaluator'''\n",
        "    return(np.sqrt(mean_squared_error(np.array(y1),np.array(y2))))\n",
        "\n",
        "def nearest(y_predicted):\n",
        "\n",
        "    y_original=y_unique\n",
        "    modified_prediction = np.zeros_like(y_predicted)\n",
        "\n",
        "    for i, y_pred in enumerate(y_predicted):\n",
        "        nearest_value = min(y_original, key=lambda x: abs(x - y_pred))\n",
        "        modified_prediction[i] = nearest_value\n",
        "\n",
        "    return modified_prediction\n",
        "\n",
        "global y_unique\n",
        "y_unique=train[target].unique()\n",
        "y_unique_log=np.log1p(train[target]).unique()\n",
        "\n",
        "xgb_params = {\n",
        "            'n_estimators': 500,\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.0116,\n",
        "            'colsample_bytree': 1,\n",
        "            'subsample': 0.6085,\n",
        "            'min_child_weight': 9,\n",
        "            'reg_lambda': 4.879e-07,\n",
        "            'max_bin': 431,\n",
        "            'n_jobs': -1,\n",
        "            'eval_metric': 'mae',\n",
        "            'objective': \"reg:absoluteerror\",\n",
        "            'tree_method': 'hist',\n",
        "            'verbosity': 0,\n",
        "            'random_state': 42,\n",
        "        }\n",
        "\n",
        "def fill_missing_numerical(train,test,target, max_iterations=10):\n",
        "    '''Iterative Missing Imputer: Updates filled missing values iteratively using CatBoost Algorithm'''\n",
        "    train_temp=train.copy()\n",
        "    if target in train_temp.columns:\n",
        "        train_temp=train_temp.drop(columns=target)\n",
        "\n",
        "\n",
        "    df=pd.concat([train_temp,test],axis=\"rows\")\n",
        "    df=df.reset_index(drop=True)\n",
        "    features=[ f for f in df.columns if df[f].isna().sum()>0]\n",
        "    if len(features)>0:\n",
        "        # Step 1: Store the instances with missing values in each feature\n",
        "        missing_rows = store_missing_rows(df, features)\n",
        "\n",
        "        # Step 2: Initially fill all missing values with \"Missing\"\n",
        "#         for f in features:\n",
        "#             df[f]=df[f].fillna(df[f].median())\n",
        "\n",
        "        cat_features=[f for f in df.columns if not pd.api.types.is_numeric_dtype(df[f])]\n",
        "        dictionary = {feature: [] for feature in features}\n",
        "\n",
        "        for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n",
        "            for feature in features:\n",
        "#                 print(feature)\n",
        "                # Skip features with no missing values\n",
        "                rows_miss = missing_rows[feature].index\n",
        "                replace_dict={}\n",
        "                rev_replace_dict={}\n",
        "                for col in  cat_features:\n",
        "                    df[col]=df[col].astype(str)\n",
        "                    int_cat=dict(zip(df[col].unique(),np.arange(0, df[col].nunique())))\n",
        "                    rev_int_cat=dict(zip(np.arange(0, df[col].nunique()), df[col].unique()))\n",
        "                    df[col]=df[col].replace(int_cat)\n",
        "\n",
        "                    replace_dict[col]=int_cat\n",
        "                    rev_replace_dict[col]=rev_int_cat\n",
        "\n",
        "                missing_temp = df.loc[rows_miss].copy()\n",
        "                non_missing_temp = df.drop(index=rows_miss).copy()\n",
        "                y_pred_prev=missing_temp[feature]\n",
        "                missing_temp = missing_temp.drop(columns=[feature])\n",
        "\n",
        "\n",
        "                # Step 3: Use the remaining features to predict missing values using Random Forests\n",
        "                X_train = non_missing_temp.drop(columns=[feature])\n",
        "                y_train = non_missing_temp[[feature]]\n",
        "\n",
        "#                 model1 = CatBoostRegressor(**cb_params)\n",
        "#                 model1.fit(X_train, y_train,cat_features=cat_features, verbose=False)\n",
        "\n",
        "                model2 = xgb.XGBRegressor(**xgb_params)\n",
        "                model2.fit(X_train, y_train, verbose=False)\n",
        "\n",
        "                # Step 4: Predict missing values for the feature and update all N features\n",
        "                y_pred = np.array(model2.predict(missing_temp))\n",
        "\n",
        "                df.loc[rows_miss, feature] = y_pred\n",
        "#                 error_minimize=rmse(y_pred,y_pred_prev) #mean_squared_error\n",
        "                error_minimize=np.sqrt(mean_squared_error(y_pred,y_pred_prev) )#mean_squared_error\n",
        "                dictionary[feature].append(error_minimize)  # Append the error_minimize value\n",
        "\n",
        "                for col in  cat_features:\n",
        "                    df[col]=df[col].replace(rev_int_cat)\n",
        "\n",
        "\n",
        "        for feature, values in dictionary.items():\n",
        "            values=np.array(values)/sum(values)\n",
        "            iterations = range(1, len(values) + 1)  # x-axis values (iterations)\n",
        "            plt.plot(iterations, values, label=feature)  # plot the values\n",
        "            plt.xlabel('Iterations')\n",
        "            plt.ylabel('RMSE')\n",
        "            plt.title('Minimization of RMSE with iterations')\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.show()\n",
        "        train[features] = np.array(df.iloc[:train.shape[0]][features])\n",
        "        test[features] = np.array(df.iloc[train.shape[0]:][features])\n",
        "\n",
        "    return train,test"
      ],
      "metadata": {
        "id": "8gZa4jx69RYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_features(data):\n",
        "    df=data.copy()\n",
        "\n",
        "    # Clean the weights by capping the over weights with total body weights\n",
        "    df['Shell_weight']=np.where(df['Shell_weight']>df['Whole_weight'],df['Whole_weight'],df['Shell_weight'])\n",
        "    df['Viscera_weight']=np.where(df['Viscera_weight']>df['Whole_weight'],df['Whole_weight'],df['Viscera_weight'])\n",
        "    df['Shucked_weight']=np.where(df['Shucked_weight']>df['Whole_weight'],df['Whole_weight'],df['Shucked_weight'])\n",
        "\n",
        "    # Abalone Surface area\n",
        "    df[\"surface_area\"]=df[\"Length\"]*df[\"Diameter\"]\n",
        "    df['total_area']=2*(df[\"surface_area\"]+df[\"Height\"]*df[\"Diameter\"]+df[\"Length\"]*df[\"Height\"])\n",
        "\n",
        "    # Abalone density approx\n",
        "    df['approx_density']=df['Whole_weight']/(df['surface_area']*df['Height']+1e-5)\n",
        "\n",
        "    # Abalone BMI\n",
        "    df['bmi']=df['Whole_weight']/(df['Height']**2+1e-5)\n",
        "\n",
        "    # Measurement derived\n",
        "    df[\"length_dia_ratio\"]=df['Length']/(df['Diameter']+1e-5)\n",
        "    df[\"length_height_ratio\"]=df['Length']/(df['Height']+1e-5)\n",
        "    df['shell_shuck_ratio']=df[\"Shell_weight\"]/(df[\"Shucked_weight\"]+1e-5)\n",
        "    df['shell_viscera_ratio']=df['Shell_weight']/(df['Viscera_weight']+1e-5)\n",
        "\n",
        "    df['viscera_tot_ratio']=df['Viscera_weight']/(df['Whole_weight']  +1e-5)\n",
        "    df['shell_tot_ratio']=df['Shell_weight']/(df['Whole_weight']    +1e-5)\n",
        "    df['shuck_tot_ratio']=df['Shucked_weight']/(df['Whole_weight']   +1e-5)\n",
        "    df['shell_body_ratio']=df['Shell_weight']/(df['Shell_weight']+df['Whole_weight']+1e-5)\n",
        "    df['flesh_ratio']=df['Shucked_weight']/(df['Whole_weight']+df['Shucked_weight']+1e-5)\n",
        "\n",
        "    df['inv_viscera_tot']= df['Whole_weight'] / (df['Viscera_weight']+1e-5)\n",
        "    df['inv_shell_tot']= df['Whole_weight'] /( df['Shell_weight']+1e-5)\n",
        "    df['inv_shuck_tot']= df['Whole_weight'] / (df['Shucked_weight']+1e-5)\n",
        "\n",
        "\n",
        "    # Water Loss during experiment\n",
        "    df[\"water_loss\"]=df[\"Whole_weight\"]-df[\"Shucked_weight\"]-df['Viscera_weight']-df['Shell_weight']\n",
        "    df[\"water_loss\"]=np.where(df[\"water_loss\"]<0,min(df[\"Shucked_weight\"].min(),df[\"Viscera_weight\"].min(),df[\"Shell_weight\"].min()),df[\"water_loss\"])\n",
        "    return df\n",
        "\n",
        "train=new_features(train)\n",
        "test=new_features(test)\n",
        "original=new_features(original)"
      ],
      "metadata": {
        "id": "v8E2aRbpRAqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_cols = [f for f in train.columns if train[f].dtype != 'O' and train[f].nunique()>10000]\n",
        "\n",
        "sc=MinMaxScaler()\n",
        "\n",
        "global unimportant_features\n",
        "global overall_best_score\n",
        "global overall_best_col\n",
        "unimportant_features=[]\n",
        "overall_best_score=1e5\n",
        "overall_best_col='none'\n",
        "\n",
        "# for col in cont_cols:\n",
        "#      train, test=min_max_scaler(train, test, col)\n",
        "\n",
        "def transformer(train, test,cont_cols, target):\n",
        "    '''\n",
        "    Algorithm applies multiples transformations on selected columns and finds the best transformation using a single variable model performance\n",
        "    '''\n",
        "    global unimportant_features\n",
        "    global overall_best_score\n",
        "    global overall_best_col\n",
        "    train_copy = train.copy()\n",
        "    test_copy = test.copy()\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Feature', 'Original RMSLE', 'Transformation', 'Tranformed RMSLE']\n",
        "\n",
        "    for col in cont_cols:\n",
        "        train_copy, test_copy=min_max_scaler(train_copy, test_copy, col)\n",
        "        for c in [\"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col, \"log_sqrt\"+col, \"pow_\"+col, \"pow2_\"+col]:\n",
        "            if c in train_copy.columns:\n",
        "                train_copy = train_copy.drop(columns=[c])\n",
        "\n",
        "        # Log Transformation after MinMax Scaling (keeps data between 0 and 1)\n",
        "        train_copy[\"log_\"+col] = np.log1p(train_copy[col])\n",
        "        test_copy[\"log_\"+col] = np.log1p(test_copy[col])\n",
        "\n",
        "        # Square Root Transformation\n",
        "        train_copy[\"sqrt_\"+col] = np.sqrt(train_copy[col])\n",
        "        test_copy[\"sqrt_\"+col] = np.sqrt(test_copy[col])\n",
        "\n",
        "        # Box-Cox transformation\n",
        "        combined_data = pd.concat([train_copy[[col]], test_copy[[col]]], axis=0)\n",
        "        epsilon = 1e-5\n",
        "        transformer = PowerTransformer(method='box-cox')\n",
        "#         scaled_data = transformer.fit_transform(combined_data + epsilon)\n",
        "\n",
        "#         train_copy[\"bx_cx_\" + col] = scaled_data[:train_copy.shape[0]]\n",
        "#         test_copy[\"bx_cx_\" + col] = scaled_data[train_copy.shape[0]:]\n",
        "        train_copy[\"bx_cx_\" + col] = transformer.fit_transform(train_copy[[col]]+epsilon)\n",
        "        test_copy[\"bx_cx_\" + col] = transformer.transform(test_copy[[col]]+epsilon)\n",
        "        # Yeo-Johnson transformation\n",
        "        transformer = PowerTransformer(method='yeo-johnson')\n",
        "        train_copy[\"y_J_\"+col] = transformer.fit_transform(train_copy[[col]])\n",
        "        test_copy[\"y_J_\"+col] = transformer.transform(test_copy[[col]])\n",
        "\n",
        "        # Power transformation, 0.25\n",
        "        power_transform = lambda x: np.power(x + 1 - np.min(x), 0.25)\n",
        "        transformer = FunctionTransformer(power_transform)\n",
        "        train_copy[\"pow_\"+col] = transformer.fit_transform(train_copy[[col]])\n",
        "        test_copy[\"pow_\"+col] = transformer.transform(test_copy[[col]])\n",
        "\n",
        "        # Power transformation, 2\n",
        "        power_transform = lambda x: np.power(x + 1 - np.min(x), 2)\n",
        "        transformer = FunctionTransformer(power_transform)\n",
        "        train_copy[\"pow2_\"+col] = transformer.fit_transform(train_copy[[col]])\n",
        "        test_copy[\"pow2_\"+col] = transformer.transform(test_copy[[col]])\n",
        "\n",
        "        # Log to power transformation\n",
        "        train_copy[\"log_sqrt\"+col] = np.log1p(train_copy[\"sqrt_\"+col])\n",
        "        test_copy[\"log_sqrt\"+col] = np.log1p(test_copy[\"sqrt_\"+col])\n",
        "\n",
        "        temp_cols = [col, \"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col,  \"pow_\"+col , \"pow2_\"+col,\"log_sqrt\"+col]\n",
        "\n",
        "        train_copy,test_copy = fill_missing_numerical(train_copy,test_copy,target,3)\n",
        "#         train_copy[temp_cols] = train_copy[temp_cols].fillna(0)\n",
        "#         test_copy[temp_cols] = test_copy[temp_cols].fillna(0)\n",
        "\n",
        "        pca = TruncatedSVD(n_components=1)\n",
        "        x_pca_train = pca.fit_transform(train_copy[temp_cols])\n",
        "        x_pca_test = pca.transform(test_copy[temp_cols])\n",
        "        x_pca_train = pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb\"])\n",
        "        x_pca_test = pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb\"])\n",
        "        temp_cols.append(col+\"_pca_comb\")\n",
        "\n",
        "        test_copy = test_copy.reset_index(drop=True)\n",
        "\n",
        "        train_copy = pd.concat([train_copy, x_pca_train], axis='columns')\n",
        "        test_copy = pd.concat([test_copy, x_pca_test], axis='columns')\n",
        "\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        rmse_scores = []\n",
        "\n",
        "        for f in temp_cols:\n",
        "            X = train_copy[[f]].values\n",
        "            y = train_copy[target].values\n",
        "\n",
        "            rmses = []\n",
        "            for train_idx, val_idx in kf.split(X, y):\n",
        "                X_train, y_train = X[train_idx], y[train_idx]\n",
        "                x_val, y_val = X[val_idx], y[val_idx]\n",
        "                model=LinearRegression()\n",
        "                model.fit(X_train,np.log1p(y_train))\n",
        "                y_pred=nearest(np.expm1(model.predict(x_val)))\n",
        "                rmses.append(rmse(np.log1p(y_val),np.log1p(y_pred)))\n",
        "            rmse_scores.append((f,np.mean(rmses)))\n",
        "\n",
        "            if overall_best_score > np.mean(rmses):\n",
        "                overall_best_score = np.mean(rmses)\n",
        "                overall_best_col = f\n",
        "\n",
        "            if f == col:\n",
        "                orig_rmse = np.mean(rmses)\n",
        "\n",
        "        best_col, best_rmse = sorted(rmse_scores, key=lambda x: x[1], reverse=False)[0]\n",
        "        cols_to_drop = [f for f in temp_cols if f != best_col]\n",
        "        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n",
        "\n",
        "        if cols_to_drop:\n",
        "            unimportant_features = unimportant_features+cols_to_drop\n",
        "        table.add_row([col,orig_rmse,best_col ,best_rmse])\n",
        "    print(table)\n",
        "    print(\"overall best CV RMSLE score: \",overall_best_score)\n",
        "    return train_copy, test_copy\n",
        "\n",
        "train, test= transformer(train, test,cont_cols, target)\n",
        "train, test=fill_missing_numerical(train,test,target, max_iterations=3)"
      ],
      "metadata": {
        "id": "TcCx6arFhXfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = PrettyTable()\n",
        "table.field_names = ['Cluster WOE Feature', 'RMSLE (CV-TRAIN)']\n",
        "for col in cont_cols:\n",
        "    sub_set=[f for f in unimportant_features if col in f]\n",
        "#     print(sub_set)\n",
        "    temp_train=train[sub_set]\n",
        "    temp_test=test[sub_set]\n",
        "    sc=StandardScaler()\n",
        "    temp_train=sc.fit_transform(temp_train)\n",
        "    temp_test=sc.transform(temp_test)\n",
        "    model = KMeans()\n",
        "\n",
        "    # print(ideal_clusters)\n",
        "    kmeans = KMeans(n_clusters=5)\n",
        "    kmeans.fit(np.array(temp_train))\n",
        "    labels_train = kmeans.labels_\n",
        "\n",
        "    train[col+\"_unimp_cluster_WOE\"] = labels_train\n",
        "    test[col+\"_unimp_cluster_WOE\"] = kmeans.predict(np.array(temp_test))\n",
        "\n",
        "    kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    X=train[[col+\"_unimp_cluster_WOE\"]].values\n",
        "    y=train[target].values\n",
        "\n",
        "    best_rmse=[]\n",
        "    for train_idx, val_idx in kf.split(X,y):\n",
        "        X_train,y_train=X[train_idx],y[train_idx]\n",
        "        x_val,y_val=X[val_idx],y[val_idx]\n",
        "        model=LinearRegression()\n",
        "        model.fit(X_train,np.log1p(y_train))\n",
        "        y_pred=nearest(np.expm1(model.predict(x_val)))\n",
        "        best_rmse.append(rmse(np.log1p(y_val),np.log1p(y_pred)))\n",
        "\n",
        "    table.add_row([col+\"_unimp_cluster_WOE\",np.mean(best_rmse)])\n",
        "    if overall_best_score<np.mean(best_rmse):\n",
        "            overall_best_score=np.mean(best_rmse)\n",
        "            overall_best_col=col+\"_unimp_cluster_WOE\"\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "id": "_oE6oECFhb40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [f for f in test.columns if (train[f].dtype != 'O' and train[f].nunique()<2000 and train[f].nunique()>2 and \"WOE\" not in f) or (train[f].dtype == 'O') ]\n",
        "print(train[cat_cols].nunique())"
      ],
      "metadata": {
        "id": "Z6hBT1qXzmL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_val(target):\n",
        "    return min(common, key=lambda x: abs(x - target))\n",
        "\n",
        "cat_cols_updated=['Sex']\n",
        "for col in cat_cols:\n",
        "    if train[col].dtype!=\"O\":\n",
        "        train[f\"{col}_cat\"]=train[col]global overall_best_score\n",
        "# overall_best_score = 0\n",
        "def OHE(train_df,test_df,cols,target):\n",
        "    '''\n",
        "    Function for one hot encoding, it first combines the data so that no category is missed and\n",
        "    the category with least frequency can be dropped because of redundancy\n",
        "    '''\n",
        "    combined = pd.concat([train_df, test_df], axis=0)\n",
        "    for col in cols:\n",
        "        one_hot = pd.get_dummies(combined[col])\n",
        "        counts = combined[col].value_counts()\n",
        "        min_count_category = counts.idxmin()\n",
        "        one_hot = one_hot.drop(min_count_category, axis=1)\n",
        "        one_hot.columns=[str(f)+col+\"_OHE\" for f in one_hot.columns]\n",
        "        combined = pd.concat([combined, one_hot], axis=\"columns\")\n",
        "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
        "\n",
        "    # split back to train and test dataframes\n",
        "    train_ohe = combined[:len(train_df)]\n",
        "    test_ohe = combined[len(train_df):]\n",
        "    test_ohe.reset_index(inplace=True,drop=True)\n",
        "    test_ohe.drop(columns=[target],inplace=True)\n",
        "    return train_ohe, test_ohe\n",
        "\n",
        "def high_freq_ohe(train, test, extra_cols, target, n_limit=50):\n",
        "    '''\n",
        "    If you wish to apply one hot encoding on a feature with so many unique values, then this can be applied,\n",
        "    where it takes a maximum of n categories and drops the rest of them treating as rare categories\n",
        "    '''\n",
        "    train_copy=train.copy()\n",
        "    test_copy=test.copy()\n",
        "    ohe_cols=[]\n",
        "    for col in extra_cols:\n",
        "        dict1=train_copy[col].value_counts().to_dict()\n",
        "        ordered=dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n",
        "        rare_keys=list([*ordered.keys()][n_limit:])\n",
        "#         ext_keys=[f[0] for f in ordered.items() if f[1]<50]\n",
        "        rare_key_map=dict(zip(rare_keys, np.full(len(rare_keys),9999)))\n",
        "\n",
        "        train_copy[col]=train_copy[col].replace(rare_key_map)\n",
        "        test_copy[col]=test_copy[col].replace(rare_key_map)\n",
        "    train_copy, test_copy = OHE(train_copy, test_copy, extra_cols, target)\n",
        "    drop_cols=[f for f in train_copy.columns if \"9999\" in f or train_copy[f].nunique()==1]\n",
        "    train_copy=train_copy.drop(columns=drop_cols)\n",
        "    test_copy=test_copy.drop(columns=drop_cols)\n",
        "\n",
        "    return train_copy, test_copy\n",
        "\n",
        "def cat_encoding(train, test,cat_cols_updated, target):\n",
        "    global overall_best_score\n",
        "    global overall_best_col\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Feature', 'Encoded Features', 'RMSLE Score']\n",
        "    train_copy=train.copy()\n",
        "    test_copy=test.copy()\n",
        "    train_dum = train.copy()\n",
        "    for feature in cat_cols_updated:\n",
        "#         print(feature)\n",
        "#         cat_labels = train_dum.groupby([feature])[target].mean().sort_values().index\n",
        "#         cat_labels2 = {k: i for i, k in enumerate(cat_labels, 0)}\n",
        "#         train_copy[feature + \"_target\"] = train[feature].map(cat_labels2)\n",
        "#         test_copy[feature + \"_target\"] = test[feature].map(cat_labels2)\n",
        "\n",
        "        dic = train[feature].value_counts().to_dict()\n",
        "        train_copy[feature + \"_count\"] =train[feature].map(dic)\n",
        "        test_copy[feature + \"_count\"] = test[feature].map(dic)\n",
        "\n",
        "        dic2=train[feature].value_counts().to_dict()\n",
        "#         list1=np.arange(len(dic2.values()),0,-1) # Higher rank for high count\n",
        "        list1=np.arange(len(dic2.values())) # Higher rank for low count\n",
        "        dic3=dict(zip(list(dic2.keys()),list1))\n",
        "\n",
        "        train_copy[feature+\"_count_label\"]=train[feature].replace(dic3).astype(float)\n",
        "        test_copy[feature+\"_count_label\"]=test[feature].replace(dic3).astype(float)\n",
        "\n",
        "        temp_cols = [ feature + \"_count\", feature + \"_count_label\"]#,feature + \"_target\"\n",
        "\n",
        "        train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n",
        "        test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n",
        "\n",
        "        if train_copy[feature].nunique()<=15:\n",
        "            train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n",
        "            test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n",
        "            train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n",
        "\n",
        "        else:\n",
        "            train_copy,test_copy=high_freq_ohe(train_copy,test_copy,[feature], target, n_limit=15)\n",
        "\n",
        "        train_copy=train_copy.drop(columns=[feature])\n",
        "        test_copy=test_copy.drop(columns=[feature])\n",
        "\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        rmse_scores = []\n",
        "\n",
        "        for f in temp_cols:\n",
        "            X = train_copy[[f]].values\n",
        "            y = train_copy[target].astype(int).values\n",
        "\n",
        "            rmses = []\n",
        "            for train_idx, val_idx in kf.split(X, y):\n",
        "                X_train, y_train = X[train_idx], y[train_idx]\n",
        "                x_val, y_val = X[val_idx], y[val_idx]\n",
        "                model=LinearRegression()\n",
        "                model.fit(X_train,np.log1p(y_train))\n",
        "                y_pred=nearest(np.expm1(model.predict(x_val)))\n",
        "                rmses.append(rmse(np.log1p(y_val),np.log1p(y_pred)))\n",
        "            rmse_scores.append((f,np.mean(rmses)))\n",
        "            if overall_best_score > np.mean(rmses):\n",
        "                overall_best_score = np.mean(rmses)\n",
        "                overall_best_col = f\n",
        "        best_col, best_auc = sorted(rmse_scores, key=lambda x: x[1], reverse=False)[0]\n",
        "\n",
        "        corr = train_copy[temp_cols].corr(method='pearson')\n",
        "        corr_with_best_col = corr[best_col]\n",
        "        cols_to_drop = [f for f in temp_cols if corr_with_best_col[f] > 0.5 and f != best_col]\n",
        "        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n",
        "        if cols_to_drop:\n",
        "            train_copy = train_copy.drop(columns=cols_to_drop)\n",
        "            test_copy = test_copy.drop(columns=cols_to_drop)\n",
        "\n",
        "        table.add_row([feature, best_col, best_auc])\n",
        "        print(feature)\n",
        "    print(table)\n",
        "    print(\"overall best CV score: \", overall_best_score)\n",
        "    return train_copy, test_copy\n",
        "\n",
        "train, test= cat_encoding(train, test,cat_cols_updated, target)\n",
        "train, test=fill_missing_numerical(train,test,target, max_iterations=3)\n",
        "\n",
        "        test[f\"{col}_cat\"]=test[col]\n",
        "        cat_cols_updated.append(f\"{col}_cat\")\n",
        "        uncommon=list((set(test[col].unique())| set(train[col].unique()))-(set(test[col].unique())& set(train[col].unique())))\n",
        "        if uncommon:\n",
        "            common=list(set(test[col].unique())& set(train[col].unique()))\n",
        "            train[f\"{col}_cat\"]=train[col].apply(nearest_val)\n",
        "            test[f\"{col}_cat\"]=test[col].apply(nearest_val)\n",
        "print(train[cat_cols_updated].nunique())"
      ],
      "metadata": {
        "id": "_fZIpo1izoXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_drop=[ f for f in unimportant_features if f in train.columns]\n",
        "train=train.drop(columns=first_drop)\n",
        "test=test.drop(columns=first_drop)"
      ],
      "metadata": {
        "id": "At2InmBGztT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_drop_list=[]\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = ['Original', 'Final Transformation', \"RMSLE(CV)- Regression\"]\n",
        "dt_params={'criterion': 'absolute_error'}\n",
        "threshold=0.85\n",
        "# It is possible that multiple parent features share same child features, so store selected features to avoid selecting the same feature again\n",
        "best_cols=[]\n",
        "\n",
        "for col in cont_cols:\n",
        "    sub_set=[f for f in train.columns if col in f and train[f].nunique()>100]\n",
        "    print(sub_set)\n",
        "    if len(sub_set)>2:\n",
        "        correlated_features = []\n",
        "\n",
        "        for i, feature in enumerate(sub_set):\n",
        "            # Check correlation with all remaining features\n",
        "            for j in range(i+1, len(sub_set)):\n",
        "                correlation = np.abs(train[feature].corr(train[sub_set[j]]))\n",
        "                # If correlation is greater than threshold, add to list of highly correlated features\n",
        "                if correlation > threshold:\n",
        "                    correlated_features.append(sub_set[j])\n",
        "\n",
        "        # Remove duplicate features from the list\n",
        "        correlated_features = list(set(correlated_features))\n",
        "        print(correlated_features)\n",
        "        if len(correlated_features)>=2:\n",
        "\n",
        "            temp_train=train[correlated_features]\n",
        "            temp_test=test[correlated_features]\n",
        "            #Scale before applying PCA\n",
        "            sc=StandardScaler()\n",
        "            temp_train=sc.fit_transform(temp_train)\n",
        "            temp_test=sc.transform(temp_test)\n",
        "\n",
        "            # Initiate PCA\n",
        "            pca=TruncatedSVD(n_components=1)\n",
        "            x_pca_train=pca.fit_transform(temp_train)\n",
        "            x_pca_test=pca.transform(temp_test)\n",
        "            x_pca_train=pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb_final\"])\n",
        "            x_pca_test=pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb_final\"])\n",
        "            train=pd.concat([train,x_pca_train],axis='columns')\n",
        "            test=pd.concat([test,x_pca_test],axis='columns')\n",
        "\n",
        "            # Clustering\n",
        "            model = KMeans()\n",
        "            kmeans = KMeans(n_clusters=28)\n",
        "            kmeans.fit(np.array(temp_train))\n",
        "            labels_train = kmeans.labels_\n",
        "\n",
        "            train[col+'_final_cluster'] = labels_train\n",
        "            test[col+'_final_cluster'] = kmeans.predict(np.array(temp_test))\n",
        "\n",
        "            cat_labels=cat_labels=train.groupby([col+\"_final_cluster\"])[target].mean()\n",
        "            cat_labels2=cat_labels.to_dict()\n",
        "            train[col+\"_final_cluster\"]=train[col+\"_final_cluster\"].map(cat_labels2)\n",
        "            test[col+\"_final_cluster\"]=test[col+\"_final_cluster\"].map(cat_labels2)\n",
        "\n",
        "            correlated_features=correlated_features+[col+\"_pca_comb_final\",col+\"_final_cluster\"]\n",
        "\n",
        "            # See which transformation along with the original is giving you the best univariate fit with target\n",
        "            kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "            rmse_scores = []\n",
        "\n",
        "            for f in temp_cols:\n",
        "                X = train_copy[[f]].values\n",
        "                y = train_copy[target].astype(int).values\n",
        "\n",
        "                rmses = []\n",
        "                for train_idx, val_idx in kf.split(X, y):\n",
        "                    X_train, y_train = X[train_idx], y[train_idx]\n",
        "                    x_val, y_val = X[val_idx], y[val_idx]\n",
        "                    model=LinearRegression()\n",
        "                    model.fit(X_train,np.log1p(y_train))\n",
        "                    y_pred=nearest(np.expm1(model.predict(x_val)))\n",
        "                    rmses.append(rmse(np.log1p(y_val),np.log1p(y_pred)))\n",
        "\n",
        "                if f not in best_cols:\n",
        "                    rmse_scores.append((f,np.mean(rmses)))\n",
        "            best_col, best_rmse=sorted(rmse_scores, key=lambda x:x[1], reverse=False)[0]\n",
        "            best_cols.append(best_col)\n",
        "\n",
        "            cols_to_drop = [f for f in correlated_features if  f not in best_cols]\n",
        "            if cols_to_drop:\n",
        "                final_drop_list=final_drop_list+cols_to_drop\n",
        "            table.add_row([col,best_col ,best_acc])\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "id": "Qp6Z9zDAVgjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_features=[f for f in train.columns if f not in [target]]\n",
        "final_features=[*set(final_features)]\n",
        "\n",
        "sc=StandardScaler()\n",
        "\n",
        "train_scaled=train.copy()\n",
        "test_scaled=test.copy()\n",
        "train_scaled[final_features]=sc.fit_transform(train[final_features])\n",
        "test_scaled[final_features]=sc.transform(test[final_features])\n",
        "len(final_features)"
      ],
      "metadata": {
        "id": "JnQkeyeEVjuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_processor(train, test):\n",
        "    cols=train.drop(columns=[target]).columns\n",
        "    train_cop=train.copy()\n",
        "    test_cop=test.copy()\n",
        "    drop_cols=[]\n",
        "    for i, feature in enumerate(cols):\n",
        "        for j in range(i+1, len(cols)):\n",
        "            if sum(abs(train_cop[feature]-train_cop[cols[j]]))==0:\n",
        "                if cols[j] not in drop_cols:\n",
        "                    drop_cols.append(cols[j])\n",
        "    print(drop_cols)\n",
        "    train_cop.drop(columns=drop_cols,inplace=True)\n",
        "    test_cop.drop(columns=drop_cols,inplace=True)\n",
        "\n",
        "    return train_cop, test_cop\n",
        "\n",
        "train_cop, test_cop=   post_processor(train_scaled, test_scaled)"
      ],
      "metadata": {
        "id": "g-jFhKJrVnjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=train_cop[train_cop[target].isin([7,8,9,10,11])]\n",
        "data_=train_cop[~train_cop[target].isin([7,8,9,10,11])]\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "def isolation_forest(data):\n",
        "    data_pure=data.copy()\n",
        "\n",
        "#     model = IsolationForest(contamination=0.010650427383702971, random_state=0)\n",
        "    model = IsolationForest(contamination=0.01, random_state=0)\n",
        "\n",
        "    drop_cols=[f for f in data_pure.columns if f in target]\n",
        "    model.fit(data_pure.drop(columns=drop_cols))\n",
        "\n",
        "    # Predict the anomaly scores for each data point\n",
        "    anomalies = model.predict(data_pure.drop(columns=drop_cols))\n",
        "\n",
        "    outliers = anomalies == -1\n",
        "\n",
        "    # Combine the outlier information with the original data and labels\n",
        "    data_pure['outlier_ISF'] = outliers\n",
        "\n",
        "    # Print the identified outliers\n",
        "    identified_outliers = data_pure[data_pure['outlier_ISF']]\n",
        "    print(f\"Number of detected Potential outliers: {identified_outliers.shape[0]}\")\n",
        "\n",
        "    data_clean=data[~data_pure['outlier_ISF']]\n",
        "    print(data_clean.shape)\n",
        "\n",
        "    return data_clean\n",
        "\n",
        "\n",
        "data_clean=isolation_forest(data).reset_index(drop=True)\n",
        "\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# params={'n_neighbors': 13, 'contamination': 0.01019797151100069}\n",
        "params={'n_neighbors': 13, 'contamination': 0.01}\n",
        "\n",
        "\n",
        "def lof(data):\n",
        "    data_pure=data.copy()\n",
        "    drop_cols=[f for f in data_pure.columns if f in target]\n",
        "\n",
        "    features = data_pure.drop(columns=drop_cols)\n",
        "    lof = LocalOutlierFactor(**params)  # Adjust contamination based on your data\n",
        "    anomalies = lof.fit_predict(features)  # Negative scores are outliers\n",
        "\n",
        "    outliers = anomalies == -1\n",
        "\n",
        "    # Combine the outlier information with the original data and labels\n",
        "    data_pure['outliers_LOF'] = outliers\n",
        "\n",
        "    # Print the identified outliers\n",
        "    identified_outliers = data_pure[data_pure['outliers_LOF']]\n",
        "    print(f\"Number of detected Potential outliers: {identified_outliers.shape[0]}\")\n",
        "\n",
        "    data_clean=data[~data_pure['outliers_LOF']]\n",
        "    print(data_clean.shape)\n",
        "\n",
        "    return data_clean\n",
        "\n",
        "data_clean=lof(data_clean).reset_index(drop=True)\n",
        "\n",
        "def pca_anamolies(data):\n",
        "    data_pure=data.copy()\n",
        "    drop_cols=[f for f in data_pure.columns if f in target]\n",
        "\n",
        "    features = data_pure.drop(columns=drop_cols)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "    pca = PCA(n_components=2)  # Choose the number of components for visualization\n",
        "    principal_components = pca.fit_transform(scaled_features)\n",
        "\n",
        "    # Calculate the reconstruction error (MSE) for each data point\n",
        "    reconstruction_errors = ((scaled_features - pca.inverse_transform(principal_components)) ** 2).mean(axis=1)\n",
        "\n",
        "    # Set a threshold for anomaly detection\n",
        "    threshold = 3.5 # Adjust the threshold based on your data and desired sensitivity\n",
        "\n",
        "    # Identify potential outliers\n",
        "    potential_outliers = [index for index, error in enumerate(reconstruction_errors) if error > threshold]\n",
        "\n",
        "    # Create a new column 'outliers' in the DataFrame\n",
        "    data_pure['outliers_PCA'] = False\n",
        "    data_pure.loc[potential_outliers, 'outliers_PCA'] = True\n",
        "\n",
        "    print(f\"Number of detected Potential outliers: {len(potential_outliers)}\")\n",
        "\n",
        "    data_clean=data[~data_pure['outliers_PCA']]\n",
        "    print(data_clean.shape)\n",
        "\n",
        "    # Plot the data with potential outliers highlighted\n",
        "    plt.scatter(principal_components[:, 0], principal_components[:, 1], c='green', label='Normal Data')\n",
        "    plt.scatter(principal_components[potential_outliers, 0], principal_components[potential_outliers, 1], c='red', label='Potential Outliers')\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.legend()\n",
        "    plt.title('PCA with Potential Outliers')\n",
        "    plt.show()\n",
        "    return data_clean\n",
        "\n",
        "data_clean=pca_anamolies( data_clean).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "-ZNowQ0hVpGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_cop.drop(columns=[target])\n",
        "y_train = train_cop[target]\n",
        "\n",
        "X_test = test_cop.copy()\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "yFhXlO_aVuUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_important_features(X_train, y_train, n,model_input):\n",
        "\n",
        "    lgb_params = {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': 6,\n",
        "            \"num_leaves\": 16,\n",
        "            'learning_rate': 0.05,\n",
        "            'subsample': 0.7,\n",
        "            'colsample_bytree': 0.8,\n",
        "            #'reg_alpha': 0.25,\n",
        "            'reg_lambda': 5e-07,\n",
        "            'objective': 'regression_l2',\n",
        "            'metric': 'mean_absolute_error',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'random_state': 42,\n",
        "            'verbose':-1\n",
        "        }\n",
        "    cb_params = {\n",
        "            'iterations': 300,\n",
        "            'depth': 6,\n",
        "            'learning_rate': 0.01,\n",
        "            'l2_leaf_reg': 0.5,\n",
        "            'random_strength': 0.2,\n",
        "            'max_bin': 150,\n",
        "            'od_wait': 80,\n",
        "            'one_hot_max_size': 70,\n",
        "            'grow_policy': 'Depthwise',\n",
        "            'bootstrap_type': 'Bayesian',\n",
        "            'od_type': 'IncToDec',\n",
        "            'eval_metric': 'MSLE',\n",
        "            'loss_function': 'RMSE',\n",
        "            'random_state': 42,\n",
        "             'verbose':False\n",
        "        }\n",
        "\n",
        "    xgb_params = {\n",
        "            'n_estimators': 500,\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.0116,\n",
        "            'colsample_bytree': 1,\n",
        "            'min_child_weight': 9,\n",
        "            'n_jobs': -1,\n",
        "            'eval_metric': 'rmsle',\n",
        "            'objective': \"reg:squarederror\",\n",
        "            'tree_method': 'hist',\n",
        "            'verbosity': 0,\n",
        "            'random_state': 42,\n",
        "        }\n",
        "    if 'xgb' in model_input:\n",
        "        model = xgb.XGBRegressor(**xgb_params)\n",
        "    elif 'cat' in model_input:\n",
        "        model=CatBoostRegressor(**cb_params)\n",
        "    else:\n",
        "        model=lgb.LGBMRegressor(**lgb_params)\n",
        "\n",
        "\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rmse_scores = []\n",
        "\n",
        "    for train_idx, val_idx in kfold.split(X_train,y_train):\n",
        "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        if 'lgb' in model_input:\n",
        "            model.fit(X_train_fold, np.log1p(y_train_fold))\n",
        "        else:\n",
        "            model.fit(X_train_fold, np.log1p(y_train_fold),verbose=False)\n",
        "\n",
        "        y_pred = model.predict(X_val_fold)\n",
        "\n",
        "        rmses = np.sqrt(mean_squared_error(np.log1p(y_val_fold),y_pred))\n",
        "        rmse_scores.append(rmses)\n",
        "\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "\n",
        "    feature_importances = model.feature_importances_\n",
        "\n",
        "    feature_importance_list = [(X_train.columns[i], importance) for i, importance in enumerate(feature_importances)]\n",
        "\n",
        "    sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    top_n_features = [feature[0] for feature in sorted_features[:n]]\n",
        "    print(avg_rmse)\n",
        "    return top_n_features\n"
      ],
      "metadata": {
        "id": "ylZqTZrRWA44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_imp_features_cat=get_most_important_features(X_train.reset_index(drop=True), y_train,75, 'cat')\n",
        "n_imp_features_xgb=get_most_important_features(X_train.reset_index(drop=True), y_train,75, 'xgb')\n",
        "n_imp_features_lgbm=get_most_important_features(X_train.reset_index(drop=True), y_train, 75, 'lgbm')"
      ],
      "metadata": {
        "id": "OlnUSTqzaAXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_imp_features=[*set(n_imp_features_lgbm+n_imp_features_cat)]#\n",
        "print(f\"{len(n_imp_features)} features have been selected from three algorithms for the final model\")\n",
        "\n",
        "X_train=X_train[n_imp_features]\n",
        "X_test=X_test[n_imp_features]"
      ],
      "metadata": {
        "id": "nuy_S9FuaAZq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}